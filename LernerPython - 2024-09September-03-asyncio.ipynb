{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `asyncio`\n",
    "\n",
    "1. What is (and isn't?) `asyncio`\n",
    "2. How it works, using non-`asyncio` stuff\n",
    "3. Coroutines and tasks and `async def`\n",
    "4. Running coroutines and `await`\n",
    "5. Task groups\n",
    "6. Getting results (and exceptions)\n",
    "7. Task pools\n",
    "8. Retrieving URLs\n",
    "9. Chat server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The background\n",
    "\n",
    "Two related terms in programming:\n",
    "\n",
    "- Concurrency -- a more general term meaning that we can benefit from pieces of our program executing semi-independently, even if they're not truly indepent of one another\n",
    "- Parallelism -- running multiple parts of our program in parallel\n",
    "\n",
    "For example:\n",
    "- If I want read from 10 different files, then I might want to use parallelism -- each core on my system can read a different file\n",
    "- If I want to download data from 10 different URLs, this also might be possible with parallelism\n",
    "- If I have only one core, but I'm reading from 10 different files, I can still use concurrency -- because I can ask for data from one file, and while that data is coming to me, I can then turn to another file and ask for its data. This works because I know that it'll take a while for the data to come from any one of those files.\n",
    "\n",
    "Note that all of these examples are for problems that are I/O-bound, meaning that the bottleneck is basically that we're reading from disks/networks/etc.\n",
    "\n",
    "What if I wanted to perform a very big, difficult calculation like MD5 or SHA1, or bitcoin mining? Can I break any of those into pieces, and have them shared across CPUs? No. Those are CPU-bound problems. \n",
    "\n",
    "Over the years, we've had two main ways to get concurrency/parallelism in Python:\n",
    "- Multiprocessing -- allows us to start new processes, split our work across them, and even join the results together. The good news is that each process is indeed separate and runs in parallel. There are two problems -- first of all, each process has a lot of overhead. The other problem is that they are indeed totally separate processes, so we have to get the data to and from each of them.\n",
    "- Threading -- we can, in Python, start lots of new threads (many more than we can start processes). Threads have far lower overhead than processes, because we're inside of a single process. Because we're in a single process, that means we can share data.  But we're in a single process, and all of the threads run on one core. In Python, because of the GIL (global interpreter lock), only one thread can run at a time. We have concurrency, but we don't have parallelism."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
